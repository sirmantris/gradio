# -*- coding: utf-8 -*-
"""gradiotest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sfvVN_eXWsQJ8jhTyvJSlmUu-_16MCDT
"""

!pip install fastai --upgrade -q
!pip install gradio
!pip install spacy
!python -m spacy download en_core_web_md
!pip install nltk
!pip install autocorrect
!pip install PySimpleGUI

import gradio as gr
import pandas as pd
import random
import nltk
from autocorrect import Speller
from gensim.summarization import summarize as g_sumn
import sys
import re
import PySimpleGUI as sg
nltk.download('punkt')
nltk.download('wordnet')
import spacy
from spacy.lang.en import English
nlp = spacy.load("en_core_web_md")

"""## Debugging"""

import gradio as gr


def calculator(num1, operation, num2):
    if operation == "add":
        return num1 + num2
    elif operation == "subtract":
        return num1 - num2
    elif operation == "multiply":
        return num1 * num2 + 1
    elif operation == "divide":
        return num1 / num2


gr.Interface(
    calculator,
    ["number", gr.inputs.Radio(["add", "subtract", "multiply", "divide"]), "number"],
    "number",
    title="test calculator",
    description="heres a sample toy calculator. enjoy!",
    flagging_options=["Cálculo Incorrecto", "Cálculo Vacío", "NA"], flagging_dir="/content/flagged"
).launch()



"""## Reports   """

def fraud_detector(card_activity, categories):
    activity_range = random.randint(1,100) #replace with model
    drop_columns = [
        column for column in ["retail", "food", "other"] if column not in categories
    ]
    if len(drop_columns):
        card_activity.drop(columns=drop_columns, inplace=True)
    return (
        card_activity,
        card_activity,
        {"fraud": activity_range / 100.0, "not fraud": 1 - activity_range / 100.0},
    )


iface = gr.Interface(
    fraud_detector,
    [gr.inputs.Timeseries(x="time", y=["retail", "food", "other"]), 
     gr.inputs.CheckboxGroup(["retail", "food", "other"], default=["retail", "food", "other"]),],
    ["dataframe", gr.outputs.Timeseries(x="time", y=["retail", "food", "other"]), gr.outputs.Label(label="Fraud Level"),], 
    allow_flagging = "never"
)
iface.launch()

"""## What if analysis"""

def inference(age, sex, ch, cardio):
    s = 0 if sex=='female' else 1
    
    prob = random.randint(1,20)/100 #replace with a real model

    df = pd.DataFrame([[age, s, ch, cardio]], 
                      columns=['Age', 'Sex', 'Cholestoral (in mg/dl)', 
                               'Resting electrocardiographic results'])

    res = {'No Heart Desease': prob, 'Has Heart Desease': 1-prob}
    return res

sex = gr.inputs.Radio(['female', 'male'], label="Sex")
age = gr.inputs.Slider(minimum=1, maximum=100, default=22, label="Age")
ch = gr.inputs.Slider(minimum=120, maximum=560, default=200, label="Cholestoral (in mg/dl)")
cardio = gr.inputs.Radio([0, 1, 2], label="Resting electrocardiographic results")

gr.Interface(inference, [age, sex, ch, cardio], "label", live=True, allow_flagging = "never").launch(share=True)

"""##Portfolio"""

def nlp2(operation,text):
    if operation == 'Lower Case':
       return text.lower()

    if operation == 'Word Tokenize':
        doc = nlp(text)
        result = {
            "result": [token.text for token in doc]  # remove str() if you want the output as list
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Lemmatize':
        from nltk.stem import WordNetLemmatizer
        wordnet_lemmatizer = WordNetLemmatizer()

        word_tokens = nltk.word_tokenize(text)
        lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in
                           word_tokens]
        result = {
            "result": " ".join(lemmatized_word)
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Stemming':
        from nltk.stem import WordNetLemmatizer
        wordnet_lemmatizer = WordNetLemmatizer()

        word_tokens = nltk.word_tokenize(text)
        lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in
                           word_tokens]
        result = {
            "result": " ".join(lemmatized_word)
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Remove Numbers':
        remove_num = ''.join(c for c in text if not c.isdigit())
        result = {
            "result": remove_num
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Remove Punctuation':
        from string import punctuation
        def strip_punctuation(s):
            return ''.join(c for c in s if c not in punctuation)

        text = strip_punctuation(text)
        result = {
            "result": text
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Spell Check':
        spell = Speller(lang='en')
        spells = [spell(w) for w in (nltk.word_tokenize(text))]
        result = {
            "result": " ".join(spells)
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Remove Stopwords':
        from nltk.corpus import stopwords
        stopword = stopwords.words('english')
        word_tokens = nltk.word_tokenize(text)
        removing_stopwords = [word for word in word_tokens if word not in stopword]
        result = {
            "result": " ".join(removing_stopwords)
        }
        result = {str(key): value for key, value in result.items()}
        return result['result']
    if operation == 'Remove Tags':
        import re
        cleaned_text = re.sub('<[^<]+?>', '', text)
        result = {
            "result": cleaned_text
        }
        result = {str(key): value for key, value in result.items()}
        res = re.sub(' +', ' ', result['result'])
        return res


iface = gr.Interface(nlp2,[gr.inputs.Dropdown(['Lower Case',
                                                  'Lemmatize',
                                                  'Stemming',
                                                  'Spell Check',
                                                  'Remove Tags',
                                                  'Word Tokenize',
                                                  'Remove Punctuation',
                                                  'Remove Numbers',
                                                  'Remove Stopwords',
                                                  ], label="Tasks"),
                           gr.inputs.Textbox(lines=1, placeholder=None, default="", label="Input text", optional=False)],"text", 
                     examples=[
        ["Lower Case", "The purpose of life is"],
        ["Lemmatize","Best coffee is made"],
        ["Summarize","What is life"],
        ["Stemming","The history never repeats but it rhymes"], 
    ],allow_flagging = "never", title="NLP demos",
    description='My work:',article='<p style="text-align:center">https://github.com/account_name</p>')

iface.launch()

"""## Annotation"""

import requests
import tensorflow as tf

import gradio as gr
a=0
inception_net = tf.keras.applications.MobileNetV2()  # load the model

# Download human-readable labels for ImageNet.
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")


def classify_image(inp):
    global a
    inp = inp.reshape((-1, 224, 224, 3))
    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)
    prediction = inception_net.predict(inp).flatten()
    a = a + 3
    return {labels[i]: float(prediction[i]) for i in range(1000)}


image = gr.inputs.Image(shape=(224, 224))
label = gr.outputs.Label(num_top_classes=3)

gr.Interface(
    fn=classify_image,
    inputs=image,
    outputs=label,
    live=True,
    flagging_options=["Correct", "Incorrect"],
    examples=[["img" + str(a) + ".png"], ["img" + str(a+1) + ".png"], ["img" + str(a+2) + ".png"], ["img" + str(a+3) + ".png"]],
).launch()